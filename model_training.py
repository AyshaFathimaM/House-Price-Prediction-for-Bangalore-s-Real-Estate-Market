# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_eDyoAvWri8HMvPf-KsOO_gdrRwRgUtw
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from data_processing import load_data, preprocess_data

def train_model(file_path):
    """
    Train and tune the model, then evaluate performance.

    Args:
        file_path (str): Path to the dataset CSV file.
    """
    # Step 1: Load and preprocess the data
    print("Loading and preprocessing data...")
    raw_data = load_data(file_path)
    data = preprocess_data(raw_data)

    # Step 2: Separate features and target
    print("Separating features and target variable...")
    X = data.drop("price", axis=1)
    y = data["price"]

    # Step 3: Define categorical and numerical features
    categorical_features = ["area_type", "location", "size", "society"]
    numeric_features = ["total_sqft", "bath", "balcony"]

    # Step 4: Create a preprocessing pipeline
    print("Creating preprocessing pipeline...")
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numeric_features),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ]
    )

    # Step 5: Split data into training and testing sets
    print("Splitting data into training and testing sets...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 6: Define the model pipeline
    print("Defining model pipeline...")
    model_pipeline = Pipeline(
        steps=[
            ("preprocessor", preprocessor),
            ("model", XGBRegressor(objective="reg:squarederror", random_state=42, n_jobs=-1)),
        ]
    )

    # Step 7: Hyperparameter Tuning
    param_grid = {
        "model__n_estimators": [200, 300, 400, 500],
        "model__learning_rate": [0.01, 0.05, 0.1, 0.2],
        "model__max_depth": [3, 5, 7, 9],
        "model__subsample": [0.8, 0.9, 1.0],
        "model__colsample_bytree": [0.8, 0.9, 1.0],
        "model__min_child_weight": [1, 5, 10],
        "model__gamma": [0, 0.1, 0.2],
    }

    print("Performing hyperparameter tuning...")
    random_search = RandomizedSearchCV(
        estimator=model_pipeline,
        param_distributions=param_grid,
        n_iter=20,  # Number of iterations for tuning
        scoring="neg_mean_squared_error",
        cv=3,
        verbose=1,
        random_state=42,
        n_jobs=-1,
    )
    random_search.fit(X_train, y_train)

    # Step 8: Get the best model
    best_pipeline = random_search.best_estimator_
    print(f"Best Parameters: {random_search.best_params_}")

    # Step 9: Evaluate the model using Cross-Validation for better generalization
    print("Evaluating the best model with Cross-Validation...")
    cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring="r2")
    print(f"Cross-validation R-squared: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # Step 10: Test the model on the test set
    print("Evaluating the model on the test set...")
    y_pred = best_pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error: {mse:.2f}")
    print(f"R-squared Score: {r2:.4f}")

    # Step 11: Save the trained pipeline
    print("Saving the best model pipeline...")
    import pickle

    with open("best_model_pipeline.pkl", "wb") as file:
        pickle.dump(best_pipeline, file)

    print("Model training and saving complete.")

if __name__ == "__main__":
    # Specify the path to the dataset CSV file
    file_path = r"C:\Users\Indra\Desktop\Praxis\Term 2\MLOPS\Bangalore Housing\Bengaluru_House_Data.csv"
    train_model(file_path)





